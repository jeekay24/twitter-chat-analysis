{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "df=pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                  tweet_id airline_sentiment  airline_sentiment_confidence   \n",
       "0      570306133677760513           neutral                        1.0000  \\\n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline   \n",
       "0                         NaN                        NaN  Virgin America  \\\n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold   \n",
       "0                        NaN          cairdin                 NaN  \\\n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text   \n",
       "0                  0                @VirginAmerica What @dhepburn said.  \\\n",
       "1                  0  @VirginAmerica plus you've added commercials t...   \n",
       "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
       "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
       "...              ...                                                ...   \n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location   \n",
       "0             NaN  2015-02-24 11:35:52 -0800            NaN  \\\n",
       "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "...           ...                        ...            ...   \n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "0      Eastern Time (US & Canada)  \n",
       "1      Pacific Time (US & Canada)  \n",
       "2      Central Time (US & Canada)  \n",
       "3      Pacific Time (US & Canada)  \n",
       "4      Pacific Time (US & Canada)  \n",
       "...                           ...  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  \n",
       "\n",
       "[14640 rows x 15 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfn=df[['text','airline_sentiment']]\n",
    "print(dfn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JeeKay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing url and special characters\n",
    "import re\n",
    "\n",
    "def remove_urls_special_characters(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters and punctuation except for apostrophes\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\']+', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                     text airline_sentiment\n",
       "1      @VirginAmerica plus you've added commercials t...          positive\n",
       "3      @VirginAmerica it's really aggressive to blast...          negative\n",
       "4      @VirginAmerica and it's a really big bad thing...          negative\n",
       "5      @VirginAmerica seriously would pay $30 a fligh...          negative\n",
       "6      @VirginAmerica yes, nearly every time I fly VX...          positive\n",
       "...                                                  ...               ...\n",
       "14633  @AmericanAir my flight was Cancelled Flightled...          negative\n",
       "14634         @AmericanAir right on cue with the delaysğŸ‘Œ          negative\n",
       "14635  @AmericanAir thank you we got on a different f...          positive\n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...          negative\n",
       "14638  @AmericanAir you have my money, you change my ...          negative\n",
       "\n",
       "[11541 rows x 2 columns]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541, 2)\n"
     ]
    }
   ],
   "source": [
    "dfn=dfn[dfn['airline_sentiment']!='neutral']\n",
    "print(dfn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                     text airline_sentiment\n",
       "1      @VirginAmerica plus you've added commercials t...          positive\n",
       "3      @VirginAmerica it's really aggressive to blast...          negative\n",
       "4      @VirginAmerica and it's a really big bad thing...          negative\n",
       "5      @VirginAmerica seriously would pay $30 a fligh...          negative\n",
       "6      @VirginAmerica yes, nearly every time I fly VX...          positive\n",
       "...                                                  ...               ...\n",
       "14633  @AmericanAir my flight was Cancelled Flightled...          negative\n",
       "14634         @AmericanAir right on cue with the delaysğŸ‘Œ          negative\n",
       "14635  @AmericanAir thank you we got on a different f...          positive\n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...          negative\n",
       "14638  @AmericanAir you have my money, you change my ...          negative\n",
       "\n",
       "[11541 rows x 2 columns]>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn[\"airline_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dfn.airline_sentiment.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, ..., 0, 1, 1], dtype=int64),\n",
       " Index(['positive', 'negative'], dtype='object'))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=dfn.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       "       '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       "       \"@VirginAmerica and it's a really big bad thing about it\", ...,\n",
       "       '@AmericanAir thank you we got on a different flight to Chicago.',\n",
       "       \"@AmericanAir leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc\",\n",
       "       \"@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.0.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "\n",
    "tokenizer.fit_on_texts(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequence = pad_sequences(encoded_docs, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeekay\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JeeKay\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JeeKay\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\JeeKay\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at siebert/sentiment-roberta-large-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pipe(\"hello there!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pipe(\"Very bad!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Conv1D, MaxPooling1D, Dense, Dropout, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequence, labels[0], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=5000, output_dim=64))\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=4))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - accuracy: 0.7587 - loss: 0.5163 - val_accuracy: 0.9108 - val_loss: 0.2520\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.9186 - loss: 0.1958 - val_accuracy: 0.9233 - val_loss: 0.2308\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.9628 - loss: 0.1072 - val_accuracy: 0.9147 - val_loss: 0.2541\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.9842 - loss: 0.0528 - val_accuracy: 0.9099 - val_loss: 0.2871\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.9931 - loss: 0.0256 - val_accuracy: 0.9147 - val_loss: 0.3385\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.9968 - loss: 0.0146 - val_accuracy: 0.9151 - val_loss: 0.3732\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0065 - val_accuracy: 0.9086 - val_loss: 0.4185\n",
      "Epoch 8/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.9981 - loss: 0.0090 - val_accuracy: 0.9129 - val_loss: 0.4452\n",
      "Epoch 9/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0037 - val_accuracy: 0.9117 - val_loss: 0.4671\n",
      "Epoch 10/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0046 - val_accuracy: 0.9164 - val_loss: 0.4876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b8dab35910>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Test Accuracy: 0.9164140224456787\n"
     ]
    }
   ],
   "source": [
    "cnn_scores = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Test Accuracy:\", cnn_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=5000, output_dim=64))\n",
    "lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "lstm_model.add(GlobalMaxPooling1D())\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 227ms/step - accuracy: 0.8221 - loss: 0.4580 - val_accuracy: 0.9177 - val_loss: 0.2282\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 224ms/step - accuracy: 0.9387 - loss: 0.1603 - val_accuracy: 0.9242 - val_loss: 0.2217\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 226ms/step - accuracy: 0.9689 - loss: 0.0933 - val_accuracy: 0.9216 - val_loss: 0.2199\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 230ms/step - accuracy: 0.9778 - loss: 0.0625 - val_accuracy: 0.9186 - val_loss: 0.2695\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 227ms/step - accuracy: 0.9859 - loss: 0.0469 - val_accuracy: 0.9190 - val_loss: 0.3128\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 223ms/step - accuracy: 0.9914 - loss: 0.0336 - val_accuracy: 0.9095 - val_loss: 0.3365\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 229ms/step - accuracy: 0.9910 - loss: 0.0294 - val_accuracy: 0.9104 - val_loss: 0.3649\n",
      "Epoch 8/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 225ms/step - accuracy: 0.9941 - loss: 0.0256 - val_accuracy: 0.9078 - val_loss: 0.4580\n",
      "Epoch 9/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 224ms/step - accuracy: 0.9963 - loss: 0.0154 - val_accuracy: 0.9199 - val_loss: 0.4337\n",
      "Epoch 10/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 223ms/step - accuracy: 0.9978 - loss: 0.0106 - val_accuracy: 0.9121 - val_loss: 0.4849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b897820110>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test Accuracy: 0.9120831489562988\n"
     ]
    }
   ],
   "source": [
    "lstm_scores = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"LSTM Test Accuracy:\", lstm_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BiLSTM Model\n",
    "bilstm_model = Sequential()\n",
    "bilstm_model.add(Embedding(input_dim=5000, output_dim=64))\n",
    "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "bilstm_model.add(GlobalMaxPooling1D())\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 247ms/step - accuracy: 0.8064 - loss: 0.4786 - val_accuracy: 0.9112 - val_loss: 0.2507\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 233ms/step - accuracy: 0.9420 - loss: 0.1636 - val_accuracy: 0.9242 - val_loss: 0.2244\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 244ms/step - accuracy: 0.9660 - loss: 0.0941 - val_accuracy: 0.9155 - val_loss: 0.2512\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 235ms/step - accuracy: 0.9781 - loss: 0.0694 - val_accuracy: 0.9181 - val_loss: 0.3040\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 250ms/step - accuracy: 0.9857 - loss: 0.0509 - val_accuracy: 0.9142 - val_loss: 0.3212\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 258ms/step - accuracy: 0.9898 - loss: 0.0372 - val_accuracy: 0.9117 - val_loss: 0.4083\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 245ms/step - accuracy: 0.9925 - loss: 0.0288 - val_accuracy: 0.9160 - val_loss: 0.3494\n",
      "Epoch 8/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 244ms/step - accuracy: 0.9936 - loss: 0.0198 - val_accuracy: 0.9104 - val_loss: 0.3841\n",
      "Epoch 9/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 242ms/step - accuracy: 0.9934 - loss: 0.0219 - val_accuracy: 0.9138 - val_loss: 0.4404\n",
      "Epoch 10/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 243ms/step - accuracy: 0.9944 - loss: 0.0186 - val_accuracy: 0.9121 - val_loss: 0.4248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b8ad8bdcd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM Test Accuracy: 0.9120831489562988\n"
     ]
    }
   ],
   "source": [
    "bilstm_scores = bilstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"BiLSTM Test Accuracy:\", bilstm_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = K.softmax(et)\n",
    "        at = K.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_attention_model = Sequential()\n",
    "bilstm_attention_model.add(Embedding(input_dim=5000, output_dim=64))\n",
    "bilstm_attention_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "bilstm_attention_model.add(AttentionLayer())\n",
    "bilstm_attention_model.add(Dense(64, activation='relu'))\n",
    "bilstm_attention_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_attention_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 334ms/step - accuracy: 0.7704 - loss: 0.5345 - val_accuracy: 0.8064 - val_loss: 0.5002\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 319ms/step - accuracy: 0.7873 - loss: 0.5102 - val_accuracy: 0.8458 - val_loss: 0.3537\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 344ms/step - accuracy: 0.8890 - loss: 0.2708 - val_accuracy: 0.9203 - val_loss: 0.2286\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 327ms/step - accuracy: 0.9479 - loss: 0.1483 - val_accuracy: 0.9255 - val_loss: 0.2173\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 297ms/step - accuracy: 0.9673 - loss: 0.1028 - val_accuracy: 0.9056 - val_loss: 0.2477\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 279ms/step - accuracy: 0.9648 - loss: 0.1078 - val_accuracy: 0.9186 - val_loss: 0.2654\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 265ms/step - accuracy: 0.9798 - loss: 0.0608 - val_accuracy: 0.9155 - val_loss: 0.2695\n",
      "Epoch 8/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 268ms/step - accuracy: 0.9862 - loss: 0.0490 - val_accuracy: 0.9203 - val_loss: 0.3276\n",
      "Epoch 9/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 267ms/step - accuracy: 0.9889 - loss: 0.0363 - val_accuracy: 0.9207 - val_loss: 0.3135\n",
      "Epoch 10/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 268ms/step - accuracy: 0.9916 - loss: 0.0295 - val_accuracy: 0.9151 - val_loss: 0.3819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b8ae208110>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_attention_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM with Attention Test Accuracy: 0.9151147603988647\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BiLSTM Model with Attention\n",
    "bilstm_attention_scores = bilstm_attention_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"BiLSTM with Attention Test Accuracy:\", bilstm_attention_scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
